{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-2 Transformer Model for Sarcasm Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lfrostbyte/anaconda3/envs/CS4248/lib/python3.11/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/lfrostbyte/anaconda3/envs/CS4248/lib/python3.11/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import gensim\n",
    "import re\n",
    "import copy\n",
    "import gc\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models import Word2Vec\n",
    "from tqdm import tqdm\n",
    "# https://colab.research.google.com/drive/1dMTdO5vxdVX0NA2Qe7AV9WGEy8ZH67Xn?usp=sharing#scrollTo=afcc233b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/lfrostbyte/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/lfrostbyte/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/lfrostbyte/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Download NLTK resources\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19230\n",
      "19230\n"
     ]
    }
   ],
   "source": [
    "# train_df = pd.read_json(path_or_buf=\"./Sarcasm_Headlines_Dataset.json\", lines=True)\n",
    "train_df = pd.read_json(path_or_buf=\"./Sarcasm_Headlines_Dataset.json\", lines=True)\n",
    "train_df.rename(columns={\"headline\": \"text\", \"is_sarcastic\": \"label\"}, inplace=True)\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Lowercasing\n",
    "    text = re.sub(r'\\W', ' ', text)  # Remove non-alphanumeric characters\n",
    "    tokens = word_tokenize(text)  # Tokenization\n",
    "    tokens = [word for word in tokens if word.isalpha()]  # Remove numbers and punctuation\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]  # Stopword removal\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]  # Lemmatization\n",
    "    \n",
    "    return tokens if tokens else None\n",
    "\n",
    "# # For convenience\n",
    "# train_df = train_df.head(100)\n",
    "\n",
    "# Apply preprocessing\n",
    "# train_df['processed_text'] = train_df['text'].apply(preprocess_text)\n",
    "# train_df = train_df.dropna(subset=['processed_text']) # After preprocessing, there will be some rows that are empty, delete these rows\n",
    "\n",
    "\n",
    "# Split into train, test, validation (80% train, 20% validation from train)\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    train_df['text'], train_df['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    train_texts, train_labels, test_size=0.1, random_state=42) # Ideally test_size should be 1/8 because 0.8 * 1/8 = 0.1 so its 10% for validation, but 0.08 is also fine.\n",
    "\n",
    "print(len(train_texts))\n",
    "print(len(train_labels))\n",
    "\n",
    "train_texts = train_texts.reset_index(drop=True)\n",
    "train_labels = train_labels.reset_index(drop=True)\n",
    "val_texts = val_texts.reset_index(drop=True)\n",
    "val_labels = val_labels.reset_index(drop=True)\n",
    "test_texts = test_texts.reset_index(drop=True)\n",
    "test_labels = test_labels.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        madonna's anti-trump cover of britney spears' ...\n",
       "1        chinese scientists successfully clone 2 monkey...\n",
       "2        john urschel on why kids shouldn't play footba...\n",
       "3        international criminal court announces new '3 ...\n",
       "4        pence tells emotional story of longtime friend...\n",
       "                               ...                        \n",
       "19225                  darfur, ia also in pretty bad shape\n",
       "19226    lgbtq activists organizing massive dance prote...\n",
       "19227    bill nye says empathy is necessary for human s...\n",
       "19228             white house declares war on dsl provider\n",
       "19229    marine determined to win heart, mind of at lea...\n",
       "Name: text, Length: 19230, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2Classifier(torch.nn.Module):\n",
    "    def __init__(self, hidden_size: int, num_classes: int, max_seq_len: int):\n",
    "        super(GPT2Classifier,self).__init__()\n",
    "        self.frontLayer = GPT2Model.from_pretrained('gpt2')\n",
    "        trf_out_size = hidden_size * max_seq_len\n",
    "        self.fc = torch.nn.Linear(in_features=trf_out_size, out_features=num_classes)\n",
    "        self.seq_len = max_seq_len\n",
    "\n",
    "    def forward(self, input_id, mask):\n",
    "        \"\"\"\n",
    "        Args: input_id: encoded input of ids that were sent\n",
    "        \"\"\"\n",
    "        gpt_out, _ = self.frontLayer(input_ids=input_id, attention_mask=mask, return_dict=False)\n",
    "        batch_size = gpt_out.shape[0]\n",
    "        linear_output = self.fc(gpt_out.view(batch_size, -1))\n",
    "        return linear_output\n",
    "\n",
    "class SarcasmDataset(Dataset):\n",
    "    def __init__(self, headlines, labels, tokenizer, max_length=50):\n",
    "        self.headlines = headlines\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def classes(self):\n",
    "        return self.labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.headlines)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        processed_headline = self.headlines[idx]\n",
    "        # processed_headline = \" \".join(self.headlines[idx])\n",
    "        encoded_data = self.tokenizer(processed_headline, padding='max_length', max_length=self.max_length, truncation=True, return_tensors=\"pt\")\n",
    "        return encoded_data, self.labels[idx]\n",
    "    \n",
    "def load_model(path, hidden_size, max_seq_len):\n",
    "    model = GPT2Classifier(hidden_size=hidden_size, num_classes=2, max_seq_len=max_seq_len)\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def train(model, trainData, valData, lr, max_epochs, early_stop_tol):\n",
    "    trainLoader = DataLoader(trainData, batch_size=16, shuffle=True)\n",
    "    valLoader = DataLoader(valData, batch_size=16, shuffle=True)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "        criterion = criterion.cuda()\n",
    "\n",
    "    did_not_improve_count = 0\n",
    "    best_val_score = 0\n",
    "    best_epoch = 0\n",
    "    model.train()\n",
    "    \n",
    "    for epoch_num in range(max_epochs):\n",
    "        # total_acc_train = 0\n",
    "        total_loss_train = 0\n",
    "        train_predictions = []\n",
    "        train_labels = []\n",
    "        for encoded_data, train_label in tqdm(trainLoader):\n",
    "            train_label = train_label.to(device)\n",
    "            mask = encoded_data['attention_mask'].to(device)\n",
    "            input_id = encoded_data[\"input_ids\"].squeeze(1).to(device)            \n",
    "\n",
    "            model.zero_grad()\n",
    "\n",
    "            output = model(input_id, mask)\n",
    "\n",
    "            batch_loss = criterion(output, train_label)\n",
    "            total_loss_train += batch_loss.item()\n",
    "\n",
    "            # acc = (output.argmax(dim=1) == train_label).sum().item()\n",
    "            # total_acc_train += acc\n",
    "\n",
    "            # add original labels\n",
    "            train_labels += train_label.cpu().numpy().flatten().tolist()\n",
    "            # get predicitons to list\n",
    "            train_predictions += output.argmax(dim=1).cpu().numpy().flatten().tolist()\n",
    "\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # total_acc_val = 0\n",
    "        total_loss_val = 0\n",
    "        val_predictions = []\n",
    "        val_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for val_input, val_label in valLoader:\n",
    "                val_label = val_label.to(device)\n",
    "                mask = val_input['attention_mask'].to(device)\n",
    "                input_id = val_input['input_ids'].squeeze(1).to(device)\n",
    "                \n",
    "                output = model(input_id, mask)\n",
    "                \n",
    "                batch_loss = criterion(output, val_label)\n",
    "                total_loss_val += batch_loss.item()\n",
    "                \n",
    "                # acc = (output.argmax(dim=1)==val_label).sum().item()\n",
    "                # total_acc_val += acc\n",
    "                # add original labels\n",
    "                val_labels += val_label.cpu().numpy().flatten().tolist()\n",
    "                # get predicitons to list\n",
    "                val_predictions += output.argmax(dim=1).cpu().numpy().flatten().tolist()\n",
    "            \n",
    "\n",
    "        # total_acc_val = total_acc_val/len(valData)\n",
    "        f1_train = f1_score(train_labels, train_predictions, average='macro')\n",
    "        f1_val = f1_score(val_labels, val_predictions, average='macro')\n",
    "\n",
    "        if f1_val > (best_val_score + early_stop_tol):\n",
    "            best_val_score = f1_val\n",
    "            did_not_improve_count = 0\n",
    "            best_epoch = epoch_num\n",
    "            print(f\"Saving new best val acc {best_val_score}\")\n",
    "            torch.save(copy.deepcopy(model.state_dict()), f\"./gpt2-synthData-lr{lr}-iter{best_epoch+1}-tol{early_stop_tol}-slen{model.seq_len}.pt\")\n",
    "        else:\n",
    "            did_not_improve_count += 1\n",
    "\n",
    "        if did_not_improve_count >= 10:\n",
    "            break\n",
    "\n",
    "        print(\n",
    "            f\"Epochs: {epoch_num + 1} | Train Loss: {total_loss_train/len(trainData): .3f} \\\n",
    "            | Train Score: {f1_train: .3f} \\\n",
    "             | Val Loss: {total_loss_val / len(valData): .3f} \\\n",
    "             | Val Score: {f1_val: .3f}\")\n",
    "        \n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "def evaluate(model, testLoader, length):\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    total_acc_test = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for test_input, test_label in testLoader:\n",
    "            test_label = test_label.to(device)\n",
    "            mask = test_input['attention_mask'].to(device)\n",
    "            input_id = test_input['input_ids'].squeeze(1).to(device)\n",
    "            \n",
    "            output = model(input_id, mask)\n",
    "                        \n",
    "            acc = (output.argmax(dim=1)==test_label).sum().item()\n",
    "            total_acc_test += acc\n",
    "\n",
    "            # add original labels\n",
    "            true_labels += test_label.cpu().numpy().flatten().tolist()\n",
    "            # get predictions to list\n",
    "            predictions += output.argmax(dim=1).cpu().numpy().flatten().tolist()\n",
    "    test_score = f1_score(true_labels, predictions, average='macro')\n",
    "    test_p = precision_score(true_labels, predictions, average='macro')\n",
    "    test_r = recall_score(true_labels, predictions, average='macro')\n",
    "    test_acc = total_acc_test / length\n",
    "    print(f'Test Accuracy: {test_acc: .3f}, F1 Score: {test_score: .3f}, Precision: {test_p}, Recall: {test_r}')\n",
    "    return true_labels, predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lfrostbyte/anaconda3/envs/CS4248/lib/python3.11/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "seq_len = 128\n",
    "hidden_size = 768\n",
    "val_tol = 0.01\n",
    "lr = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2Classifier(hidden_size=hidden_size, num_classes=2, max_seq_len=seq_len)\n",
    "trainData = SarcasmDataset(headlines=train_texts, labels=train_labels, tokenizer=tokenizer, max_length=seq_len)\n",
    "valData = SarcasmDataset(headlines=val_texts, labels=val_labels, tokenizer=tokenizer, max_length=seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1283/1283 [05:45<00:00,  3.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving new best val acc 0.80931297086475\n",
      "Epochs: 1 | Train Loss:  0.033             | Train Score:  0.729              | Val Loss:  0.026              | Val Score:  0.809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1283/1283 [05:50<00:00,  3.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving new best val acc 0.8382520481912606\n",
      "Epochs: 2 | Train Loss:  0.023             | Train Score:  0.834              | Val Loss:  0.023              | Val Score:  0.838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1283/1283 [06:08<00:00,  3.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 3 | Train Loss:  0.022             | Train Score:  0.839              | Val Loss:  0.021              | Val Score:  0.845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1283/1283 [06:13<00:00,  3.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 4 | Train Loss:  0.020             | Train Score:  0.855              | Val Loss:  0.022              | Val Score:  0.842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1283/1283 [06:12<00:00,  3.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 5 | Train Loss:  0.020             | Train Score:  0.859              | Val Loss:  0.023              | Val Score:  0.834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1283/1283 [06:11<00:00,  3.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving new best val acc 0.8551782811865095\n",
      "Epochs: 6 | Train Loss:  0.019             | Train Score:  0.864              | Val Loss:  0.020              | Val Score:  0.855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1283/1283 [05:58<00:00,  3.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 7 | Train Loss:  0.019             | Train Score:  0.870              | Val Loss:  0.021              | Val Score:  0.852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1283/1283 [05:50<00:00,  3.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving new best val acc 0.8653725442020088\n",
      "Epochs: 8 | Train Loss:  0.018             | Train Score:  0.870              | Val Loss:  0.019              | Val Score:  0.865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1283/1283 [05:49<00:00,  3.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 9 | Train Loss:  0.018             | Train Score:  0.879              | Val Loss:  0.021              | Val Score:  0.854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1283/1283 [05:50<00:00,  3.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 10 | Train Loss:  0.019             | Train Score:  0.864              | Val Loss:  0.022              | Val Score:  0.831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1283/1283 [05:49<00:00,  3.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 11 | Train Loss:  0.020             | Train Score:  0.863              | Val Loss:  0.020              | Val Score:  0.860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1283/1283 [05:48<00:00,  3.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 12 | Train Loss:  0.020             | Train Score:  0.856              | Val Loss:  0.023              | Val Score:  0.837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1283/1283 [05:49<00:00,  3.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 13 | Train Loss:  0.020             | Train Score:  0.862              | Val Loss:  0.023              | Val Score:  0.835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1283/1283 [05:48<00:00,  3.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 14 | Train Loss:  0.020             | Train Score:  0.854              | Val Loss:  0.022              | Val Score:  0.831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1283/1283 [05:49<00:00,  3.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 15 | Train Loss:  0.020             | Train Score:  0.859              | Val Loss:  0.022              | Val Score:  0.846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1283/1283 [05:48<00:00,  3.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 16 | Train Loss:  0.020             | Train Score:  0.862              | Val Loss:  0.022              | Val Score:  0.846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1283/1283 [05:49<00:00,  3.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 17 | Train Loss:  0.020             | Train Score:  0.861              | Val Loss:  0.023              | Val Score:  0.836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1283/1283 [05:49<00:00,  3.67it/s]\n"
     ]
    }
   ],
   "source": [
    "train(model, trainData, valData, lr, 500, val_tol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.869, F1 Score:  0.864, Precision: 0.8760050588109114, Recall: 0.8591229055620623\n",
      "5342\n"
     ]
    }
   ],
   "source": [
    "testData = SarcasmDataset(headlines=test_texts, labels=test_labels, tokenizer=tokenizer, max_length=seq_len)\n",
    "testLoader = DataLoader(testData, batch_size=16, shuffle=True)\n",
    "\n",
    "model = load_model(\"./gpt2-classifier-model-lr1e-05-iter5-tol0.01-slen128.pt\", hidden_size=768, max_seq_len=seq_len)\n",
    "\n",
    "evaluate(model, testLoader, len(testData))\n",
    "\n",
    "print(len(testData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lfrostbyte/anaconda3/envs/CS4248/lib/python3.11/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.899, F1 Score:  0.899, Precision: 0.9019334975369457, Recall: 0.8979925847457627\n",
      "5702\n"
     ]
    }
   ],
   "source": [
    "# FOR SYNTHETIC DATASET\n",
    "# Load dataset (change MyDrive to Shared if it's not the same on yours)\n",
    "train_df = pd.read_json(\"synthetic_data_v2.json\", lines=True)\n",
    "\n",
    "# Rename columns if necessary\n",
    "train_df.rename(columns={\"headline\": \"text\", \"is_sarcastic\": \"label\"}, inplace=True)\n",
    "\n",
    "# Separate generated and original\n",
    "generated_df = train_df[train_df['article_link'] == '']\n",
    "original_df = train_df[train_df['article_link'] != '']\n",
    "\n",
    "# Total size\n",
    "total_size = len(train_df)\n",
    "train_target_size = int(0.8 * total_size)\n",
    "\n",
    "# How many original samples are needed to reach 80% with generated included\n",
    "num_original_needed = train_target_size - len(generated_df)\n",
    "\n",
    "# Sample required number of original samples for training\n",
    "orig_train = original_df.sample(n=num_original_needed, random_state=42)\n",
    "\n",
    "# The rest of original data goes to validation\n",
    "test = original_df.drop(orig_train.index)\n",
    "\n",
    "# Combine generated + sampled original for final training set\n",
    "train = pd.concat([generated_df, orig_train], ignore_index=True)\n",
    "\n",
    "# Split into texts and labels\n",
    "train_texts, train_labels = train['text'], train['label']\n",
    "test_texts, test_labels = test['text'], test['label']\n",
    "\n",
    "train_texts = train_texts.reset_index(drop=True)\n",
    "train_labels = train_labels.reset_index(drop=True)\n",
    "test_texts = test_texts.reset_index(drop=True)\n",
    "test_labels = test_labels.reset_index(drop=True)\n",
    "\n",
    "testData = SarcasmDataset(headlines=test_texts, labels=test_labels, tokenizer=tokenizer, max_length=seq_len)\n",
    "testLoader = DataLoader(testData, batch_size=16, shuffle=True)\n",
    "\n",
    "model = load_model(\"./gpt2-synthData-lr1e-05-iter8-tol0.01-slen128.pt\", hidden_size=768, max_seq_len=seq_len)\n",
    "\n",
    "evaluate(model, testLoader, len(testData))\n",
    "\n",
    "print(len(testData))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS4248",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
