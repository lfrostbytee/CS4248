{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-2 Transformer Model for Sarcasm Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/lfrostbyte/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/lfrostbyte/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/lfrostbyte/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import gensim\n",
    "import re\n",
    "import copy\n",
    "import spacy\n",
    "import gc\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models import Word2Vec\n",
    "from tqdm import tqdm\n",
    "from transformers import GPT2TokenizerFast\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(spacy.prefer_gpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_link</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://local.theonion.com/mom-starting-to-fea...</td>\n",
       "      <td>mom starting to fear son's web series closest ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://politics.theonion.com/boehner-just-wan...</td>\n",
       "      <td>boehner just wants wife to listen, not come up...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://politics.theonion.com/top-snake-handle...</td>\n",
       "      <td>top snake handler leaves sinking huckabee camp...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://entertainment.theonion.com/nuclear-bom...</td>\n",
       "      <td>nuclear bomb detonates during rehearsal for 's...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.theonion.com/cosby-lawyer-asks-why...</td>\n",
       "      <td>cosby lawyer asks why accusers didn't come for...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11719</th>\n",
       "      <td>https://www.theonion.com/new-bailiff-tired-of-...</td>\n",
       "      <td>new bailiff tired of hearing how old bailiff d...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11720</th>\n",
       "      <td>https://www.theonion.com/breaking-the-onion-in...</td>\n",
       "      <td>breaking: 'the onion' in kill range of boston ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11721</th>\n",
       "      <td>https://www.theonion.com/seaworld-crowd-applau...</td>\n",
       "      <td>seaworld crowd applauds for dolphin playfully ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11722</th>\n",
       "      <td>https://politics.theonion.com/pentagon-to-with...</td>\n",
       "      <td>pentagon to withhold budget figures out of res...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11723</th>\n",
       "      <td>https://www.theonion.com/pope-francis-wearing-...</td>\n",
       "      <td>pope francis wearing sweater vestments he got ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11724 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            article_link  \\\n",
       "0      https://local.theonion.com/mom-starting-to-fea...   \n",
       "1      https://politics.theonion.com/boehner-just-wan...   \n",
       "2      https://politics.theonion.com/top-snake-handle...   \n",
       "3      https://entertainment.theonion.com/nuclear-bom...   \n",
       "4      https://www.theonion.com/cosby-lawyer-asks-why...   \n",
       "...                                                  ...   \n",
       "11719  https://www.theonion.com/new-bailiff-tired-of-...   \n",
       "11720  https://www.theonion.com/breaking-the-onion-in...   \n",
       "11721  https://www.theonion.com/seaworld-crowd-applau...   \n",
       "11722  https://politics.theonion.com/pentagon-to-with...   \n",
       "11723  https://www.theonion.com/pope-francis-wearing-...   \n",
       "\n",
       "                                                    text  label  \n",
       "0      mom starting to fear son's web series closest ...      1  \n",
       "1      boehner just wants wife to listen, not come up...      1  \n",
       "2      top snake handler leaves sinking huckabee camp...      1  \n",
       "3      nuclear bomb detonates during rehearsal for 's...      1  \n",
       "4      cosby lawyer asks why accusers didn't come for...      1  \n",
       "...                                                  ...    ...  \n",
       "11719  new bailiff tired of hearing how old bailiff d...      1  \n",
       "11720  breaking: 'the onion' in kill range of boston ...      1  \n",
       "11721  seaworld crowd applauds for dolphin playfully ...      1  \n",
       "11722  pentagon to withhold budget figures out of res...      1  \n",
       "11723  pope francis wearing sweater vestments he got ...      1  \n",
       "\n",
       "[11724 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json(path_or_buf=\"./Sarcasm_Headlines_Dataset.json\", lines=True)\n",
    "df.rename(columns={\"headline\": \"text\", \"is_sarcastic\": \"label\"}, inplace=True)\n",
    "\n",
    "# # For convenience\n",
    "# train_df = train_df.head(100)\n",
    "\n",
    "sarcastic_df = df[df['label'] == 1]\n",
    "non_sarcastic_df = df[df['label'] == 0]\n",
    "sarcastic_df = sarcastic_df.reset_index(drop=True)\n",
    "non_sarcastic_df = non_sarcastic_df.reset_index(drop=True)\n",
    "\n",
    "sarcastic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16465\n",
      "16871\n"
     ]
    }
   ],
   "source": [
    "def train_gpt2_tokenizer_from_df(df, save_path, vocab_size=52000):\n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    # Extract texts from the specified column and drop NaNs\n",
    "    texts = df[\"text\"].dropna().astype(str).tolist()\n",
    "    \n",
    "    # Save texts to a temporary file (ByteLevelBPETokenizer requires a file input)\n",
    "    with open(\"train_data.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for line in texts:\n",
    "            f.write(line.strip() + \"\\n\")\n",
    "    \n",
    "    # Train the tokenizer\n",
    "    tokenizer = ByteLevelBPETokenizer()\n",
    "    tokenizer.train(\n",
    "        files=\"train_data.txt\",\n",
    "        vocab_size=vocab_size,\n",
    "        min_frequency=2,\n",
    "        special_tokens=[\"<pad>\", \"<s>\", \"</s>\", \"<unk>\", \"<mask>\"]\n",
    "    )\n",
    "    \n",
    "    # Save the tokenizer files\n",
    "    tokenizer.save_model(save_path)\n",
    "    print(f\"Saved tokenizer in {save_path}\")\n",
    "\n",
    "def load_gpt2_tokenizer(path):\n",
    "    # Load and return as a Hugging Face-compatible tokenizer\n",
    "    return GPT2TokenizerFast.from_pretrained(path)\n",
    "\n",
    "# train_gpt2_tokenizer_from_df(sarcastic_df, save_path=\"./tokenizer_sarcastic\")\n",
    "sarcastic_tokenizer = load_gpt2_tokenizer(\"./tokenizer_sarcastic\")\n",
    "# train_gpt2_tokenizer_from_df(non_sarcastic_df, save_path=\"./tokenizer_non_sarcastic\")\n",
    "non_sarcastic_tokenizer = load_gpt2_tokenizer(\"./tokenizer_non_sarcastic\")\n",
    "\n",
    "print(sarcastic_tokenizer.vocab_size)\n",
    "print(non_sarcastic_tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19230\n",
      "19230\n"
     ]
    }
   ],
   "source": [
    "# Split into train, test, validation (80% train, 20% validation from train)\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    df['text'], df['label'], test_size=0.2, random_state=42) # Leave 20% of the dataset for Test\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    train_texts, train_labels, test_size=0.1, random_state=42) # Leave 80(0.1)=8% of the dataset for Validation\n",
    "\n",
    "print(len(train_texts))\n",
    "print(len(train_labels))\n",
    "\n",
    "# Reset the indexes (first column) to ensure that they are continuous\n",
    "train_texts = train_texts.reset_index(drop=True) \n",
    "train_labels = train_labels.reset_index(drop=True)\n",
    "val_texts = val_texts.reset_index(drop=True)\n",
    "val_labels = val_labels.reset_index(drop=True)\n",
    "test_texts = test_texts.reset_index(drop=True)\n",
    "test_labels = test_labels.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lfrostbyte/anaconda3/envs/CS4248/lib/python3.11/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2Model(\n",
      "  (wte): Embedding(50257, 768)\n",
      "  (wpe): Embedding(1024, 768)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      "  (h): ModuleList(\n",
      "    (0-11): 12 x GPT2Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2Attention(\n",
      "        (c_attn): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        (c_fc): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (act): NewGELUActivation()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(GPT2Model.from_pretrained(\"gpt2\")) # Display the architecture of our front layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleFeedTokenizerClassifier(torch.nn.Module): # Double Feed\n",
    "    def __init__(self, hidden_size: int, num_classes: int, max_seq_len: int):\n",
    "        super(DoubleFeedTokenizerClassifier,self).__init__()\n",
    "        self.frontLayer = GPT2Model.from_pretrained('gpt2')\n",
    "        self.fc = torch.nn.Linear(in_features=max_seq_len*(hidden_size + hidden_size), out_features=num_classes)\n",
    "\n",
    "    def forward(self, input_id_sarcastic, mask_sarcastic, input_id_non_sarcastic, mask_non_sarcastic):\n",
    "        \"\"\"\n",
    "        Args: input_id: encoded input of ids that were sent\n",
    "        \"\"\"\n",
    "        trf_feats_sarcastic, _ = self.frontLayer(input_ids=input_id_sarcastic, attention_mask=mask_sarcastic, return_dict=False)\n",
    "        trf_feats_non_sarcastic, _ = self.frontLayer(input_ids=input_id_non_sarcastic, attention_mask=mask_non_sarcastic, return_dict=False)\n",
    "        n = trf_feats_sarcastic.shape[0]\n",
    "        extracted_features = torch.concat((trf_feats_sarcastic, trf_feats_non_sarcastic), dim=-1) # Shape: (n, seq_len, 768 * 2)\n",
    "        extracted_features = extracted_features.view(n, -1) # Flatten features\n",
    "        linear_output = self.fc(extracted_features)\n",
    "        return linear_output\n",
    "    \n",
    "class DoubleConcatTokenizerClassifier(torch.nn.Module):\n",
    "    def __init__(self, hidden_size: int, num_classes: int, max_seq_len: int):\n",
    "        super(DoubleConcatTokenizerClassifier,self).__init__()\n",
    "        self.frontLayer = GPT2Model.from_pretrained('gpt2')\n",
    "        self.fc = torch.nn.Linear(in_features=max_seq_len*(hidden_size + hidden_size), out_features=num_classes)\n",
    "\n",
    "    def forward(self, input_id_sarcastic, mask_sarcastic, input_id_non_sarcastic, mask_non_sarcastic):\n",
    "        \"\"\"\n",
    "        Args: input_id: encoded input of ids that were sent\n",
    "        \"\"\"\n",
    "        input_ids = torch.concat((input_id_sarcastic, input_id_non_sarcastic), dim=-1)\n",
    "        masks = torch.concat((mask_sarcastic, mask_non_sarcastic), dim=-1)\n",
    "        extracted_features, _ = self.frontLayer(input_ids=input_ids, attention_mask=masks, return_dict=False)\n",
    "        extracted_features = extracted_features.view(extracted_features.shape[0], -1)\n",
    "        linear_output = self.fc(extracted_features)\n",
    "        return linear_output\n",
    "\n",
    "class SarcasmDataset(Dataset):\n",
    "    def __init__(self, headlines, labels, sarcastic_tokenizer, non_sarcastic_tokenizer, max_length=50):\n",
    "        self.headlines = headlines\n",
    "        self.labels = labels\n",
    "        self.sarcastic_tokenizer = sarcastic_tokenizer\n",
    "        self.non_sarcastic_tokenizer = non_sarcastic_tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def classes(self):\n",
    "        return self.labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.headlines)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        processed_headline = self.headlines[idx]\n",
    "        # GPT2 Tokenizer\n",
    "        encoded_data_sarcastic = self.sarcastic_tokenizer(processed_headline, padding='max_length', max_length=self.max_length, truncation=True, return_tensors=\"pt\")\n",
    "        encoded_data_non_sarcastic = self.non_sarcastic_tokenizer(processed_headline, padding='max_length', max_length=self.max_length, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "        return encoded_data_sarcastic, encoded_data_non_sarcastic, self.labels[idx]\n",
    "    \n",
    "def load_model(path, hidden_size, max_seq_len):\n",
    "    model = DoubleFeedTokenizerClassifier(hidden_size=hidden_size, num_classes=2, max_seq_len=max_seq_len) # Remember to change the class when you are using DoubleConcat!\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def train(model, trainDataSarcastic, valDataSarcastic, lr, eps, V):\n",
    "    \"\"\"\n",
    "    V = Sarcastic Vocab Size\n",
    "    \"\"\"\n",
    "    trainLoaderSarcastic = DataLoader(trainDataSarcastic, batch_size=16, shuffle=True)\n",
    "    valLoaderSarcastic = DataLoader(valDataSarcastic, batch_size=16, shuffle=True)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "        criterion = criterion.cuda()\n",
    "\n",
    "    did_not_improve_count = 0\n",
    "    best_val_score = 0\n",
    "    best_epoch = 0\n",
    "    model.train()\n",
    "    \n",
    "    for epoch_num in range(100):\n",
    "        # total_acc_train = 0\n",
    "        total_loss_train = 0\n",
    "        train_predictions = []\n",
    "        train_labels = []\n",
    "        for encoded_data_sarcastic, encoded_data_non_sarcastic, train_label in tqdm(trainLoaderSarcastic):\n",
    "            train_label = train_label.to(device)\n",
    "            \n",
    "            mask_sarcastic = encoded_data_sarcastic['attention_mask'].to(device)\n",
    "            input_id_sarcastic = encoded_data_sarcastic[\"input_ids\"].squeeze(1).to(device)\n",
    "\n",
    "            mask_non_sarcastic = encoded_data_non_sarcastic['attention_mask'].to(device)\n",
    "            input_id_non_sarcastic = encoded_data_non_sarcastic[\"input_ids\"].squeeze(1).to(device) + V # Offset the input_ids because input_id \"1\" refers to different tokens in each tokenizer.\n",
    "\n",
    "            model.zero_grad()\n",
    "\n",
    "            output = model(input_id_sarcastic, mask_sarcastic, input_id_non_sarcastic, mask_non_sarcastic)\n",
    "\n",
    "            batch_loss = criterion(output, train_label)\n",
    "            total_loss_train += batch_loss.item()\n",
    "\n",
    "            train_labels += train_label.detach().cpu().numpy().flatten().tolist()\n",
    "            train_predictions += output.argmax(dim=1).detach().cpu().numpy().flatten().tolist()\n",
    "\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # total_acc_val = 0\n",
    "        total_loss_val = 0\n",
    "        val_predictions = []\n",
    "        val_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for encoded_data_sarcastic, encoded_data_non_sarcastic, val_label in valLoaderSarcastic:\n",
    "                val_label = val_label.to(device)\n",
    "                mask_sarcastic = encoded_data_sarcastic['attention_mask'].to(device)\n",
    "                input_id_sarcastic = encoded_data_sarcastic['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "                mask_non_sarcastic = encoded_data_non_sarcastic['attention_mask'].to(device)\n",
    "                input_id_non_sarcastic = encoded_data_non_sarcastic[\"input_ids\"].squeeze(1).to(device) + V\n",
    "                \n",
    "                output = model(input_id_sarcastic, mask_sarcastic, input_id_non_sarcastic, mask_non_sarcastic)\n",
    "                \n",
    "                batch_loss = criterion(output, val_label)\n",
    "                total_loss_val += batch_loss.item()\n",
    "                \n",
    "                val_labels += val_label.detach().cpu().numpy().flatten().tolist()\n",
    "                val_predictions += output.argmax(dim=1).detach().cpu().numpy().flatten().tolist()\n",
    "            \n",
    "        f1_train = f1_score(train_labels, train_predictions, average='macro')\n",
    "        f1_val = f1_score(val_labels, val_predictions, average='macro')\n",
    "\n",
    "        if f1_val > (best_val_score + eps):\n",
    "            best_val_score = f1_val\n",
    "            did_not_improve_count = 0\n",
    "            best_epoch = epoch_num\n",
    "            print(f\"Saving new best val acc {best_val_score}\")\n",
    "            torch.save(copy.deepcopy(model.state_dict()), f\"./doubleconcat-tokenizer-gpt2-lr{lr}-iter{best_epoch+1}-tol{eps}-slen128.pt\")\n",
    "        else:\n",
    "            did_not_improve_count += 1\n",
    "\n",
    "        if did_not_improve_count >= 10:\n",
    "            break\n",
    "\n",
    "        print(\n",
    "            f\"Epochs: {epoch_num + 1} | Train Loss: {total_loss_train/len(trainDataSarcastic): .3f} \\\n",
    "            | Train Score: {f1_train: .3f} \\\n",
    "             | Val Loss: {total_loss_val / len(valDataSarcastic): .3f} \\\n",
    "             | Val Score: {f1_val: .3f}\")\n",
    "        \n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "def evaluate(model, testLoader, length, V):\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    total_acc_test = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for encoded_data_sarcastic, encoded_data_non_sarcastic, test_label in testLoader:\n",
    "            test_label = test_label.to(device)\n",
    "            mask_sarcastic = encoded_data_sarcastic['attention_mask'].to(device)\n",
    "            input_id_sarcastic = encoded_data_sarcastic['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "            mask_non_sarcastic = encoded_data_non_sarcastic['attention_mask'].to(device)\n",
    "            input_id_non_sarcastic = encoded_data_non_sarcastic[\"input_ids\"].squeeze(1).to(device) + V\n",
    "            \n",
    "            output = model(input_id_sarcastic, mask_sarcastic, input_id_non_sarcastic, mask_non_sarcastic)\n",
    "                        \n",
    "            acc = (output.argmax(dim=1)==test_label).sum().item()\n",
    "            total_acc_test += acc\n",
    "\n",
    "            # add original labels\n",
    "            true_labels += test_label.detach().cpu().numpy().flatten().tolist()\n",
    "            # get predictions to list\n",
    "            predictions += output.argmax(dim=1).detach().cpu().numpy().flatten().tolist()\n",
    "    test_score = f1_score(true_labels, predictions, average='macro')\n",
    "    test_p = precision_score(true_labels, predictions, average='macro')\n",
    "    test_r = recall_score(true_labels, predictions, average='macro')\n",
    "    test_acc = total_acc_test / length\n",
    "    print(f'Test Accuracy: {test_acc: .3f}, F1 Score: {test_score: .3f}, Precision: {test_p}, Recall: {test_r}')\n",
    "    return true_labels, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "sarcastic_tokenizer.padding_side = \"left\"\n",
    "sarcastic_tokenizer.pad_token = sarcastic_tokenizer.eos_token\n",
    "\n",
    "non_sarcastic_tokenizer.padding_side = \"left\"\n",
    "non_sarcastic_tokenizer.pad_token = non_sarcastic_tokenizer.eos_token\n",
    "seq_len = 128\n",
    "hidden_size = 768\n",
    "val_tol = 0.01\n",
    "lr = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DoubleFeedTokenizerClassifier(hidden_size=hidden_size, num_classes=2, max_seq_len=seq_len)\n",
    "trainData = SarcasmDataset(headlines=train_texts, labels=train_labels, sarcastic_tokenizer=sarcastic_tokenizer, non_sarcastic_tokenizer=non_sarcastic_tokenizer, max_length=seq_len)\n",
    "valData = SarcasmDataset(headlines=val_texts, labels=val_labels, sarcastic_tokenizer=sarcastic_tokenizer, non_sarcastic_tokenizer=non_sarcastic_tokenizer, max_length=seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1202/1202 [19:18<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving new best val acc 0.8474624197126237\n",
      "Epochs: 1 | Train Loss:  0.031             | Train Score:  0.741              | Val Loss:  0.021              | Val Score:  0.847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1202/1202 [19:45<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving new best val acc 0.885114375464638\n",
      "Epochs: 2 | Train Loss:  0.020             | Train Score:  0.855              | Val Loss:  0.017              | Val Score:  0.885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1202/1202 [20:03<00:00,  1.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 3 | Train Loss:  0.017             | Train Score:  0.881              | Val Loss:  0.018              | Val Score:  0.883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1202/1202 [21:21<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 4 | Train Loss:  0.016             | Train Score:  0.894              | Val Loss:  0.019              | Val Score:  0.875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1202/1202 [21:41<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 5 | Train Loss:  0.015             | Train Score:  0.901              | Val Loss:  0.017              | Val Score:  0.886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1202/1202 [21:43<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 6 | Train Loss:  0.014             | Train Score:  0.909              | Val Loss:  0.017              | Val Score:  0.871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1202/1202 [21:42<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 7 | Train Loss:  0.015             | Train Score:  0.902              | Val Loss:  0.016              | Val Score:  0.893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1202/1202 [21:19<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 8 | Train Loss:  0.013             | Train Score:  0.914              | Val Loss:  0.017              | Val Score:  0.885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1202/1202 [23:33<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving new best val acc 0.8957814702868864\n",
      "Epochs: 9 | Train Loss:  0.013             | Train Score:  0.913              | Val Loss:  0.016              | Val Score:  0.896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1202/1202 [21:27<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 10 | Train Loss:  0.013             | Train Score:  0.918              | Val Loss:  0.020              | Val Score:  0.869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1202/1202 [21:18<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 11 | Train Loss:  0.013             | Train Score:  0.918              | Val Loss:  0.016              | Val Score:  0.892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1202/1202 [20:54<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 12 | Train Loss:  0.013             | Train Score:  0.918              | Val Loss:  0.026              | Val Score:  0.848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1202/1202 [21:14<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 13 | Train Loss:  0.013             | Train Score:  0.917              | Val Loss:  0.017              | Val Score:  0.893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1202/1202 [23:20<00:00,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 14 | Train Loss:  0.012             | Train Score:  0.923              | Val Loss:  0.021              | Val Score:  0.864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1202/1202 [23:49<00:00,  1.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 15 | Train Loss:  0.011             | Train Score:  0.928              | Val Loss:  0.019              | Val Score:  0.880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1202/1202 [22:22<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 16 | Train Loss:  0.011             | Train Score:  0.930              | Val Loss:  0.019              | Val Score:  0.879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1202/1202 [22:49<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 17 | Train Loss:  0.010             | Train Score:  0.936              | Val Loss:  0.019              | Val Score:  0.886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1202/1202 [22:15<00:00,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 18 | Train Loss:  0.010             | Train Score:  0.937              | Val Loss:  0.017              | Val Score:  0.894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1202/1202 [23:07<00:00,  1.15s/it]\n"
     ]
    }
   ],
   "source": [
    "train(model=model, trainDataSarcastic=trainData, valDataSarcastic=valData, lr=lr, eps=val_tol, V=sarcastic_tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.904, F1 Score:  0.903, Precision: 0.9017639256549035, Recall: 0.9068206315439626\n",
      "5342\n"
     ]
    }
   ],
   "source": [
    "testData = SarcasmDataset(headlines=test_texts, labels=test_labels, sarcastic_tokenizer=sarcastic_tokenizer, non_sarcastic_tokenizer=non_sarcastic_tokenizer, max_length=seq_len)\n",
    "testLoader = DataLoader(testData, batch_size=16, shuffle=True)\n",
    "\n",
    "model = load_model(\"./doublefeed-tokenizer-gpt2-lr1e-05-iter10-tol0.01-slen128.pt\", hidden_size=768, max_seq_len=seq_len)\n",
    "\n",
    "evaluate(model, testLoader, len(testData), V=sarcastic_tokenizer.vocab_size)\n",
    "\n",
    "print(len(testData))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS4248",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
